"""
Pipeline - Etapa 4: Avaliar Modelo
(Este arquivo est√° PRONTO - n√£o precisa modificar)
"""

from sklearn.metrics import (
    f1_score, 
    accuracy_score, 
    precision_score, 
    recall_score,
    confusion_matrix,
    classification_report
)


def avaliar_modelo(modelo, X_test, y_test):
    """
    Avalia o modelo usando v√°rias m√©tricas.
    
    Args:
        modelo: modelo treinado
        X_test: features de teste
        y_test: target de teste
        
    Returns:
        Dicion√°rio com m√©tricas
    """
    
    # Fazer predi√ß√µes
    y_pred = modelo.predict(X_test)
    
    # Calcular m√©tricas
    metricas = {
        'accuracy': accuracy_score(y_test, y_pred),
        'precision': precision_score(y_test, y_pred, zero_division=0),
        'recall': recall_score(y_test, y_pred, zero_division=0),
        'f1_score': f1_score(y_test, y_pred, zero_division=0)
    }
    
    return metricas, y_pred


def exibir_resultados(metricas, y_test, y_pred):
    """
    Exibe os resultados de forma formatada.
    
    Args:
        metricas: dicion√°rio com m√©tricas
        y_test: valores reais
        y_pred: valores preditos
    """
    
    print("\n" + "=" * 50)
    print("RESULTADOS DA AVALIA√á√ÉO")
    print("=" * 50)
    
    print(f"\nüìä M√âTRICAS:")
    print(f"   Accuracy:  {metricas['accuracy']:.4f} ({metricas['accuracy']*100:.2f}%)")
    print(f"   Precision: {metricas['precision']:.4f}")
    print(f"   Recall:    {metricas['recall']:.4f}")
    print(f"   F1-Score:  {metricas['f1_score']:.4f}")
    
    print(f"\nüìã MATRIZ DE CONFUS√ÉO:")
    cm = confusion_matrix(y_test, y_pred)
    print(f"   Verdadeiros Negativos (TN): {cm[0,0]}")
    print(f"   Falsos Positivos (FP):      {cm[0,1]}")
    print(f"   Falsos Negativos (FN):      {cm[1,0]}")
    print(f"   Verdadeiros Positivos (TP): {cm[1,1]}")
    
    print("\n" + "=" * 50)
    print(f"üéØ F1-SCORE FINAL: {metricas['f1_score']:.4f}")
    print("=" * 50)
    
    return metricas['f1_score']
